---
title: Transformer
date: 2022-11-30 16:44:11
tags:
  - Transformer
categories: 
  - AIGC
  - Transformer  
---

<p></p>
<!-- more -->

## 目录
<!-- toc -->

## Attention [3]
### self-attention
{% asset_img 'self-attention.jpg' %}

aligment

## Transformer [2]
### Encoder-Decoder架构 [1]
{% asset_img 'Transformer_decoder.jpg' %}
{% asset_img 'transformer_resideual_layer_norm_3.jpg' %}



transfomer 架构在GPU上的并行

### Self-attention

### Multi-Head Attention(MHA)

###  Positional Encoding

# Transformer 变体
{% asset_img 'transformers.jpg' %}


# Attention 变体[5]
{% asset_img 'attentions.jpg' %}


# 参考
1. [illustrated-transformer](http://jalammar.github.io/illustrated-transformer/) *** 
2. [Transformer - Attention is all you need](https://zhuanlan.zhihu.com/p/311156298)
3. [超详细图解Self-Attention](https://zhuanlan.zhihu.com/p/410776234) ***
4. [李宏毅《深度学习》- Self-attention 自注意力机制](https://blog.csdn.net/kkm09/article/details/120855658)
5. [主流大语言模型的技术原理细节](https://cloud.tencent.com/developer/article/2328541)
100. [Transformers from scratch](http://arthurchiao.art/blog/transformers-from-scratch-zh/) V, github 未

