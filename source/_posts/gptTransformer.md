---
title: Transformer
date: 2022-11-30 16:44:11
tags:
  - Transformer
categories: 
  - AIGC
  - Transformer  
---

<p></p>
<!-- more -->

## 目录
<!-- toc -->

## Attention [3]
### self-attention
{% asset_img 'self-attention.jpg' %}

aligment

## Transformer [2]
### Encoder-Decoder架构 [1]
{% asset_img 'Transformer_decoder.jpg' %}
{% asset_img 'transformer_resideual_layer_norm_3.jpg' %}



transfomer 架构在GPU上的并行

### Self-attention

### Multi-Head Attention(MHA)

###  Positional Encoding

# Transformer 变体
{% asset_img 'transformers.jpg' %}


# Attention 变体[5]
{% asset_img 'attentions.jpg' %}


# 参考
1. [illustrated-transformer](http://jalammar.github.io/illustrated-transformer/) *** 
2. [Transformer - Attention is all you need](https://zhuanlan.zhihu.com/p/311156298)
3. [超详细图解Self-Attention](https://zhuanlan.zhihu.com/p/410776234) ***
4. [李宏毅《深度学习》- Self-attention 自注意力机制](https://blog.csdn.net/kkm09/article/details/120855658)
5. [主流大语言模型的技术原理细节](https://cloud.tencent.com/developer/article/2328541)
100. [Transformers from scratch](http://arthurchiao.art/blog/transformers-from-scratch-zh/) V, github 未
101. [LLM Visualization](https://bbycroft.net/llm) ***  未
102. [Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT](https://blog.csdn.net/v_JULY_v/article/details/127411638) ***
103. [从零实现Transformer的简易版与强大版：从300多行到3000多行](https://blog.csdn.net/v_JULY_v/article/details/130090649)
104. [一文通透各种注意力：从多头注意力MHA到分组查询注意力GQA、多查询注意力MQA](https://blog.csdn.net/v_JULY_v/article/details/134228287)
105. [一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long](https://blog.csdn.net/v_JULY_v/article/details/134085503)

